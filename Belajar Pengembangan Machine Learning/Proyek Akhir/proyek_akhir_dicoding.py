# -*- coding: utf-8 -*-
"""Proyek Akhir Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kRGS61rO-O3f7j6l_c3lwnA8xHyrtwDf
"""

# Nama : JIRYAN FAROKHI
# Email : jiryanfarokhi@gmail.com
# Domisili : Surabaya

# Warnings Remove
import warnings
warnings.filterwarnings("ignore")
#Import Os and Basis Libraries
import cv2
import os
import pandas as pd
import numpy as np
from tensorflow.keras.callbacks import ModelCheckpoint
import seaborn as sns
import matplotlib.pyplot as plt
#Matplot Images
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split
# Tensflor and Keras Layer and Model and Optimize and Loss
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import *
from tensorflow.keras.losses import BinaryCrossentropy
#Kernel Intilizer
from tensorflow.keras.initializers import he_normal
# import tensorflow_hub as hub
from tensorflow.keras.optimizers import Adam
#PreTrained Model VGG16
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import Xception
#Image Generator DataAugmentation
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
#Early Stopping
from tensorflow.keras.callbacks import EarlyStopping
import zipfile
import pathlib
# import splitfolders

!chmod 600 /content/kaggle.json

!KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images

zip_file = zipfile.ZipFile('/content/shoe-vs-sandal-vs-boot-dataset-15k-images.zip', 'r')
zip_file.extractall('/tmp')
zip_file.close()

base_dir = '/tmp/images'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'val')

dir = "/tmp/Shoe_vs_Sandal_vs_Boot_Dataset"

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 25,
    horizontal_flip = True,
    shear_range = 0.2,
    fill_mode = 'wrap',
    validation_split = 0.2
)

validation_datagen = ImageDataGenerator(
    rescale = 1./255,
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    dir,
    target_size = (128,128),
    shuffle = True,
    subset = 'training',
    batch_size = 32
)

validation_generator = validation_datagen.flow_from_directory(
    dir,
    target_size= (128,128),
    subset = 'validation',
    batch_size = 32
)

model_2 = Sequential(name="jiryan_CNN")

model_2.add(Conv2D(64, kernel_size=(3,3), padding ='same', activation ='relu', kernel_initializer=he_normal(), input_shape = (128,128, 3), name ='CONV_Layer1'))
model_2.add(MaxPooling2D(pool_size=(2,2), padding = 'same'))
model_2.add(BatchNormalization())
model_2.add(Conv2D(32, kernel_size = (3,3), padding = 'same', activation ='relu', kernel_initializer= he_normal(), name = 'CONV_Layer2'))
model_2.add(MaxPooling2D(pool_size = (2, 2), padding = 'same'))
model_2.add(BatchNormalization())
model_2.add(Dropout(0, 40))

model_2.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', kernel_initializer = he_normal(), name = 'CONV_Layer3'))
model_2.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))
model_2.add(BatchNormalization())

model_2.add(Dropout(0,40))

model_2.add(Flatten(name = 'Flatten'))

model_2.add(Dense(220, activation = 'relu', kernel_initializer = he_normal(), name = 'FullyConnected1'))
model_2.add(Dense(64, activation = 'relu'))

model_2.add(Dense(3, activation = 'softmax', kernel_initializer = he_normal(), name = 'OutputLayer'))

model_2.summary()

model_2.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

checkpoint_callback = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')
early_stopping_callback = EarlyStopping(min_delta = 0.001, patience=5, restore_best_weights=True, monitor='val_loss')

history = model_2.fit(
    train_generator,
    steps_per_epoch=33,
    epochs=50,
    validation_steps=5,
    validation_data=validation_generator,
    callbacks=[checkpoint_callback, early_stopping_callback]
)

# Menyimpan model dalam format SavedModel
export_dir = 'saved_model/'
tf.saved_model.save(model_2, export_dir)

# Convert SavedModel menjadi vegs.tflite
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('vegs.tflite')
tflite_model_file.write_bytes(tflite_model)

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

